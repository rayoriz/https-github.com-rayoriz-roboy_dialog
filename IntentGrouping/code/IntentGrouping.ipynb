{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Intent based Grouping for Dialogues\n",
    "\n",
    "# Outline\n",
    "\n",
    "<!-- MarkdownTOC autolink=true autoanchor=true bracket=round -->\n",
    "\n",
    "- [Preprocess Data](#part-one---Preprocess Data)\n",
    "    - [Entity Substitution](#Extract the Entities for processing)\n",
    "- [Feature Extraction](#part-two---Feature Extraction)\n",
    "    - [Word Embeddings](#Use word embedding for representing the question-utternaces)\n",
    "- [Intent Clustering](#part-three---Intent Clustering)\n",
    "- [Visualization](#part-four---Visualization)\n",
    "\n",
    "<!-- /MarkdownTOC -->\n",
    "\n",
    "First make sure you have the right version of python and the libraries. To execute or \"run\" the code use \"shift+enter\")!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are running an older version of Python!\n",
      "\n",
      "You should consider updating to Python 3.4.0 or higher as the libraries built for this course have only been tested in Python 3.4 and higher.\n",
      "\n",
      "Try installing the Python 3.5 version of anaconda and then restart `jupyter notebook`:\n",
      "https://www.continuum.io/downloads\n",
      "\n",
      "\n",
      "You do not have tensorflow installed!\n",
      "Follow the instructions on the following link\n",
      "to install tensorflow before continuing:\n",
      "\n",
      "https://www.tensorflow.org/get_started/os_setup\n"
     ]
    }
   ],
   "source": [
    "# First check the Python version\n",
    "import sys\n",
    "if sys.version_info < (3,4):\n",
    "    print('You are running an older version of Python!\\n\\n' \\\n",
    "          'You should consider updating to Python 3.4.0 or ' \\\n",
    "          'higher as the libraries built for this course ' \\\n",
    "          'have only been tested in Python 3.4 and higher.\\n')\n",
    "    print('Try installing the Python 3.5 version of anaconda '\n",
    "          'and then restart `jupyter notebook`:\\n' \\\n",
    "          'https://www.continuum.io/downloads\\n\\n')\n",
    "\n",
    "# Now get necessary libraries\n",
    "try:\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from skimage.transform import resize\n",
    "    from skimage import data\n",
    "    from scipy.misc import imresize\n",
    "    import IPython.display as ipyd\n",
    "except ImportError:\n",
    "    print('You are missing some packages! ' \\\n",
    "          'We will try installing them before continuing!')\n",
    "    !pip install \"numpy>=1.11.0\" \"matplotlib>=1.5.1\" \"scikit-image>=0.11.3\" \"scikit-learn>=0.17\" \"scipy>=0.17.0\"\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from skimage.transform import resize\n",
    "    from skimage import data\n",
    "    from scipy.misc import imresize\n",
    "    import IPython.display as ipyd\n",
    "    print('Done!')\n",
    "\n",
    "# Import Tensorflow\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except ImportError:\n",
    "    print(\"You do not have tensorflow installed!\")\n",
    "    print(\"Follow the instructions on the following link\")\n",
    "    print(\"to install tensorflow before continuing:\")\n",
    "    print(\"\")\n",
    "    print(\"https://www.tensorflow.org/get_started/os_setup\")\n",
    "\n",
    "\n",
    "# We'll tell matplotlib to inline any drawn figures like so:\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> .rendered_html code { \n",
       "    padding: 2px 4px;\n",
       "    color: #c7254e;\n",
       "    background-color: #f9f2f4;\n",
       "    border-radius: 4px;\n",
       "} </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Formatting to change the default inline code style:\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"<style> .rendered_html code { \n",
    "    padding: 2px 4px;\n",
    "    color: #c7254e;\n",
    "    background-color: #f9f2f4;\n",
    "    border-radius: 4px;\n",
    "} </style>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color='blue'>Functions to do entity extraction related tasks</font></h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entity_extraction(utterance):\n",
    "    \"\"\"Extract entity from an utterance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    utterance : str\n",
    "        Utterance containing an entity.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    entity : str\n",
    "        Extracted entity from the utterance.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use NLP toolkits to extract entity out of the utterance.\n",
    "    \n",
    "    return entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A Knowledge Base to represent the generic high level semantic concept for each entity\n",
    "knowledge_base = {'roboy':'person',\n",
    "                  'Roboy':'person',\n",
    "                  'radio':'appliance',\n",
    "                 'remote':'appliance',\n",
    "                 'ac':'appliance',\n",
    "                 'cooler':'appliance',\n",
    "                 'fans':'appliance',\n",
    "                 'music player':'appliance',\n",
    "                 'air conditioner':'appliance',\n",
    "                 'mp3':'appliance',\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def entity_substitution(utterance, entity, operation):\n",
    "    \"\"\"Manipulate the extracted entity in an utterance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    utterance : str\n",
    "        Utterance to be processed.\n",
    "    entity : str\n",
    "        Entity in the utterance to be operated on.\n",
    "    operation : str\n",
    "        Type of operation to be performed with the entity - remove, substitute or same.\n",
    "        remove     - Remove the entity from the utterance and return it.\n",
    "        substitute - Substitute the entity from the utterance by generic high level term and return it.\n",
    "        same       - Return the utterance as it is.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ent_subs_utterance : str\n",
    "        Processed utterance by performing the opeartion on the entity in it.\n",
    "    \"\"\"\n",
    "    \n",
    "    if operation == 'same':\n",
    "        ent_subs_utterance = utterance\n",
    "    elif operation == 'remove':\n",
    "        ent_subs_utterance = utterance.replace(entity, '')\n",
    "    elif operation == 'substitute':\n",
    "        # Query a Knowledge Base to substitute with the generic entity\n",
    "        generic_entity = knowledge_base[entity]\n",
    "        ent_subs_utterance = utterance.replace(entity, generic_entity) \n",
    "        \n",
    "    \n",
    "    return ent_subs_utterance\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function to preprocess the dataset\n",
    "def preprocess(ds):\n",
    "    processed_utterance = []\n",
    "    \n",
    "#     Define the type of operation to be performed with the entity - remove, substitute or same.\n",
    "#     operation = 'same'\n",
    "    operation = 'remove'\n",
    "    \n",
    "    for utterance in ds:\n",
    "        entity = entity_extraction(utterance)\n",
    "        ent_subs_utterance = entity_substitution(utterance, entity, operation)\n",
    "        processed_utterance.append(ent_subs_utterance)\n",
    "        \n",
    "    return processed_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How old are you, Roboy?\n",
      "How old are you, ?\n",
      "How old are you, person?\n"
     ]
    }
   ],
   "source": [
    "ent_subs_utterance = entity_substitution('How old are you, Roboy?', 'Roboy', 'same')\n",
    "print(ent_subs_utterance)\n",
    "ent_subs_utterance = entity_substitution('How old are you, Roboy?', 'Roboy', 'remove')\n",
    "print(ent_subs_utterance)\n",
    "ent_subs_utterance = entity_substitution('How old are you, Roboy?', 'Roboy', 'substitute')\n",
    "print(ent_subs_utterance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color='purple'>Functions to extract features</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_embeddings(glove_path):\n",
    "    \"\"\"Compute an index mapping words to known embeddings, \n",
    "       by parsing the data dump of pre-trained embeddings of 100 dimensions for each word.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    glove_path : str\n",
    "        Path where the GLOVE pretrained dump is kept.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    embeddings_index : dictionary\n",
    "        Embedding vector for each word.\n",
    "    \"\"\"\n",
    "    \n",
    "    embeddings_index = {}\n",
    "    f = open(os.path.join(glove_path, 'glove.6B.200d.txt'))\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vishal\\tensorflow\\IntentGrouping\\DialogSystem\\IntentGrouping\\dataset\\glove.6B\n"
     ]
    }
   ],
   "source": [
    "glove_path = r'C:\\Users\\Vishal\\tensorflow\\IntentGrouping\\DialogSystem\\IntentGrouping\\dataset\\glove.6B'\n",
    "print(glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = load_embeddings(glove_path)\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "text_samples = ['Can you please tell me how old are you?',\n",
    "                'What is your age?',\n",
    "                'Please tell me your age.',\n",
    "                'Please tell me how old are you?',\n",
    "                'What is your age, please?',\n",
    "                'Say me your age?',\n",
    "                'How old are you?',\n",
    "                'Roboy, how old are you?',\n",
    "                'When were you born?',\n",
    "                'What\\'s the next birthday you celebrate?',\n",
    "                \n",
    "                'Who are you?',\n",
    "                'Please tell me your name.',\n",
    "                'What is your name?',\n",
    "                'Tell me your name, please.',\n",
    "                'How to call you?',\n",
    "                'How can I call you?',\n",
    "                'Do you have a name?',\n",
    "                'What\\'s the name you\\'ve been given?',\n",
    "                \n",
    "                'Who are your creators?',\n",
    "                'Where were you born?',\n",
    "                'Who created you?',\n",
    "                'Where were you assembled?',\n",
    "                'Can you please let us know where were you assembled?',\n",
    "                'Can you please let us know who created you?',\n",
    "                'Can you let us know who created you?',\n",
    "                'Do you know who developed you?',\n",
    "                'Tell me how was you created?',\n",
    "                'Whose your daddy?',\n",
    "                'Where are you from?',\n",
    "                \n",
    "                'What can you do?',\n",
    "                'What are you skilled at?',\n",
    "                'How can you help me?',\n",
    "                'Please tell me what can you do',\n",
    "                'What did you learn?',\n",
    "                'Can you really talk and understand or you just pretend?',\n",
    "                'What are you capable of?',\n",
    "                'What actions can you perform?',\n",
    "                'Can you tell me about your functions?',\n",
    "                'What are your abilities?',\n",
    "                \n",
    "                'How did you learn stuff?',\n",
    "                'How have you come to this?',\n",
    "                'From where did you learn things?',\n",
    "                'Who teaches you all this?',\n",
    "                'Who is your teacher?',\n",
    "                'Who is your guru?',\n",
    "                'Who is your mentor?',\n",
    "                'How have you learnt all your skills?',\n",
    "                'How do you know how to do stuff?',\n",
    "                \n",
    "                'How do you like this conference?',\n",
    "                'Do you like this fair?',\n",
    "                'What are your thoughts about the current exhibition?',\n",
    "                'How is this evening for you?',\n",
    "                'Do you feel comfortable here?',\n",
    "                'Are you going to stay till the end?',\n",
    "                'Why are you here?',\n",
    "                \n",
    "                'How is the weather today?',\n",
    "                'What is best thing to do in the city today?',\n",
    "                'Are you aware of any spectacular shows coming soon?',\n",
    "                'Name me some historically important places?',\n",
    "                'How is the road situation today?',\n",
    "                'Any suggestions on how to spend the day?',\n",
    "                'How far did you travel to this city?',\n",
    "                \n",
    "                'What is your goal?',\n",
    "                'What do you live for?',\n",
    "                'What were you created for?',\n",
    "                'Why do you speak with people?',\n",
    "                'What is your purpose?',\n",
    "                'What is the purpose of your existence?',\n",
    "                'What is the main reason for your creation?',\n",
    "                'What is the sole purpose of your existence?',\n",
    "                'Why are you so special?',\n",
    "                                                \n",
    "                \n",
    "                # All above utterances will be used for clustering of intents.\n",
    "                # Sample utterances below used for predicting after clustering\n",
    "                'How old are you, Roboy?',\n",
    "                'Tell me your name.',\n",
    "                'Can you let me know who created you?',\n",
    "                'What are your functions?',\n",
    "                'Who taught you all the skills?',\n",
    "                'What would you say about this meeting?',\n",
    "                'Could you recommend some place to visit?',\n",
    "                'Why were you created?'    \n",
    "]\n",
    "\n",
    "# text_samples = preprocess(text_samples)\n",
    "# print(text_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n",
      "[ 0.85395002  0.57146001 -0.023652   -0.11047    -0.1275      0.085129\n",
      " -0.74975997 -0.068121    0.043576    0.63928002 -0.072301    0.28775001\n",
      "  0.66308999  0.23428001 -0.080206    0.27575001 -0.30429     0.36572999\n",
      "  0.14379001 -0.107       0.33497     3.0789001   0.059451    0.039004\n",
      "  0.45506999 -0.47595    -0.10247     0.28632    -0.31316999 -0.053495\n",
      " -0.17990001 -0.075404    0.11216    -0.098163   -0.10058    -0.33414\n",
      " -0.52158999 -0.17538001  0.008864    0.30078     0.083636    0.38332999\n",
      " -0.12608001  0.47973001 -0.33916     0.34158     1.02139997 -0.15933999\n",
      "  0.09167     0.42668     0.30746999 -0.10632     0.051894    0.49204999\n",
      "  0.48486     0.026916    0.091038   -0.30983999 -0.12899999  0.14038999\n",
      "  0.093296   -0.057087   -0.058724   -0.27043    -0.36083999 -0.11826\n",
      " -0.013159    0.67659998  0.56496     0.10306     0.89587998 -0.083035\n",
      "  0.20385    -0.31218001 -0.78539002 -0.17744    -0.95660001 -0.18685\n",
      " -0.65925997  0.16091    -0.12383     0.023029    0.08357     0.16348\n",
      " -0.066267   -0.46505001 -0.42498001 -0.28439999  0.37591001 -1.52409995\n",
      "  0.58921999 -0.050554    0.53356999  0.26322001  0.047219    0.13751\n",
      "  0.061737   -0.043889   -0.50959998 -0.2766      0.14519     0.20367\n",
      " -0.15443    -0.042115    0.66834998  0.068898   -0.38482001  1.78680003\n",
      " -0.45886001  0.19063     0.15545     0.10557     0.17447001 -0.15255\n",
      " -0.40628001 -0.37698999  0.53288001 -0.025475   -0.50638998 -0.089237\n",
      "  0.80524999 -0.22465999  0.40134001 -0.52478999 -0.36886001 -0.12123\n",
      "  0.25624999 -0.031981    0.41846001 -0.53333002 -0.68976998  0.37097001\n",
      "  0.13459    -0.17697001 -0.23334999 -0.09276     0.26003999 -0.022291\n",
      "  0.21098    -0.40865001  0.0071628  -0.35877001  0.27792001 -0.37053001\n",
      "  1.58510005  0.37796     0.27895999 -0.48423001 -0.12335     0.028271\n",
      "  1.08589995  0.86023998 -0.82690001  0.54018003  0.044138    0.16097\n",
      " -0.23442     0.083067    0.19395     0.25347999 -0.070216    0.27421999\n",
      "  0.079368   -0.60442001  0.15406001 -0.18175    -0.030669    0.20611\n",
      " -0.57564998 -0.35120001 -0.32589    -0.1055      0.49399     0.34931999\n",
      "  0.15188     0.20423999 -0.42785999  0.070315    0.32806     0.50217003\n",
      "  1.10539997 -0.47244999 -0.40336001  0.094475   -0.24575    -0.49691001\n",
      "  0.14487     0.055714    0.12388    -0.075973   -0.47409001 -0.31062999\n",
      "  0.10934    -0.045062   -0.11885    -0.21459     0.21724001  0.31083\n",
      " -0.22303     0.20370001]\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "word_index = count_vect.fit_transform(text_samples)\n",
    "# print(word_index)\n",
    "# Perform operations on word_index to convert it into a dictionary.\n",
    "embedding_dim = 200\n",
    "print(word_index.shape[1])\n",
    "\n",
    "print(embeddings_index.get('you'))\n",
    "\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "# tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "# Remove all punctuation symbols, special characters and convert all words to lower case\n",
    "\n",
    "word_index = text_samples\n",
    "# Perform operations on word_index to convert it into a dictionary.\n",
    "embedding_dim = 200\n",
    "# No of words that each utterance should contain.\n",
    "no_of_words = 10\n",
    "print(len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_embedding_matrix(word_index, embeddings_index):\n",
    "    \"\"\"Leverage the embedding_index dictionary and word_index to compute the embedding matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word_index : dictionary\n",
    "        Words tokenized from a list of text samples\n",
    "        \n",
    "    embeddings_index : dictionary\n",
    "        Index mapping words to known embeddings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    embedding_matrix : matrix\n",
    "        Embedding matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index), no_of_words, embedding_dim))\n",
    "    for i, sentence in enumerate(word_index):\n",
    "        for j, word in enumerate(sentence.split()):\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be set to all-zeros.\n",
    "                embedding_matrix[i,j] = embedding_vector\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79L, 10L, 200L)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = get_embedding_matrix(word_index, embeddings_index)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to flatten out word embeddings (3D) into sentence embedding (2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentence_embedding(embedding_matrix):\n",
    "    \"\"\"Function to flatten out word embeddings (3D) into sentence embedding (2D) for clustering and t-SNE.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding_matrix : matrix\n",
    "        Embedding matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sentence_rep : matrix\n",
    "        Embedding matrix for each each sentence combining all words in it.\n",
    "    \"\"\"\n",
    "    sentence_rep = []\n",
    "    for row in embedding_matrix:\n",
    "        sentence_embedding = [item for sublist in row for item in sublist]\n",
    "        sentence_embedding = np.nan_to_num(sentence_embedding)\n",
    "        sentence_embedding[sentence_embedding == np.inf] = 0\n",
    "        sentence_embedding[sentence_embedding == -np.inf] = 0\n",
    "        sentence_embedding[sentence_embedding == np.nan] = 0\n",
    "    #     print(sentence_embedding)\n",
    "    #     sentence_rep = (tsne.fit_transform(sentence_embedding))\n",
    "        sentence_rep.append(sentence_embedding)\n",
    "    \n",
    "    return sentence_rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color='teal'>Functions to cluster the data</font></h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "K means clustering on both supervised and unsupervised data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import distance\n",
    "# - cluster it into k clusters (e.g., k=100)\n",
    "\n",
    "k = 8 #1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_rep = []\n",
    "sentence_rep = get_sentence_embedding(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = sentence_rep[0:-8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Repeat number of runs with different seeds and use different initialization technique.\n",
    "# clusterKMeans = KMeans(n_clusters=k, n_init=1, init='random')\n",
    "clusterKMeans = KMeans(n_clusters=k, n_init=10, init='k-means++') #, max_iter=300, tol=0.0001, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusterKMeans.fit(X)\n",
    "clusterNewCentroids = clusterKMeans.cluster_centers_\n",
    "clusterKMeansLabels = clusterKMeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "yPred1 = clusterKMeans.predict(sentence_rep[-1])\n",
    "yPred2 = clusterKMeans.predict(sentence_rep[-2])\n",
    "yPred3 = clusterKMeans.predict(sentence_rep[-3])\n",
    "yPred4 = clusterKMeans.predict(sentence_rep[-4])\n",
    "yPred5 = clusterKMeans.predict(sentence_rep[-5])\n",
    "yPred6 = clusterKMeans.predict(sentence_rep[-6])\n",
    "yPred7 = clusterKMeans.predict(sentence_rep[-7])\n",
    "yPred8 = clusterKMeans.predict(sentence_rep[-8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]),\n",
       " array([5, 2, 6, 3, 2, 2, 0, 0, 1, 3, 0, 6, 2, 2, 0, 0, 6, 0, 1, 1, 0, 1, 5,\n",
       "        5, 3, 3, 3, 0, 1, 1, 1, 6, 4, 1, 4, 1, 6, 3, 1, 6, 3, 6, 6, 2, 2, 2,\n",
       "        3, 4, 3, 6, 1, 3, 6, 3, 1, 0, 4, 3, 3, 3, 4, 4, 2, 6, 1, 3, 2, 7, 7,\n",
       "        7, 1]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterNewCentroids, clusterKMeansLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1]),\n",
       " array([3]),\n",
       " array([3]),\n",
       " array([3]),\n",
       " array([1]),\n",
       " array([3]),\n",
       " array([2]),\n",
       " array([0]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yPred1, yPred2, yPred3, yPred4, yPred5, yPred6, yPred7, yPred8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color='orange'>Functions to visualize the data</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.io import output_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -8.21051702e+00,  -1.37725659e+02],\n",
       "       [  3.08547158e+01,  -2.98444632e+01],\n",
       "       [ -9.84644823e+01,  -4.91917304e+00],\n",
       "       [ -4.13665439e+01,   7.21082656e+01],\n",
       "       [  1.14932044e+00,  -7.89672128e+00],\n",
       "       [  3.45952766e+01,   6.51432817e+01],\n",
       "       [ -9.58103188e+01,   4.30395507e+01],\n",
       "       [  1.23579315e+02,  -2.81136309e+01],\n",
       "       [  9.28406087e+01,   1.57504676e+01],\n",
       "       [  3.40117812e+01,  -2.16240200e+02],\n",
       "       [ -1.07370980e+02,   1.70555502e+02],\n",
       "       [  4.37907675e+01,  -8.89103584e+01],\n",
       "       [  2.37774406e+01,   2.07740055e+01],\n",
       "       [ -2.85752770e+01,   1.23357033e+02],\n",
       "       [ -2.02971948e+01,  -4.87911187e+00],\n",
       "       [ -8.73905106e+01,  -6.18450946e+01],\n",
       "       [ -6.17392715e+01,  -5.39488002e+01],\n",
       "       [ -7.40685216e+01,   1.18813109e+02],\n",
       "       [  7.41775704e+01,   1.11943378e+01],\n",
       "       [  9.03241936e+01,  -8.07342496e+00],\n",
       "       [  5.55610135e+01,   2.90255582e+01],\n",
       "       [  2.31001965e+01,  -1.34309432e+01],\n",
       "       [  3.18542817e+01,   4.02409316e+01],\n",
       "       [ -3.80389002e+01,   3.59384170e+01],\n",
       "       [ -9.35638316e+00,  -8.81355270e+01],\n",
       "       [ -7.87333805e+01,  -3.57926914e+01],\n",
       "       [ -2.00668560e+01,  -3.40387897e+01],\n",
       "       [ -4.14263685e+01,  -1.34759033e+01],\n",
       "       [  8.99011435e+01,   3.70521766e+01],\n",
       "       [  5.10062496e+01,  -2.64539861e+01],\n",
       "       [  7.58338417e+01,   5.20948223e+01],\n",
       "       [  2.47657197e+01,  -1.04806904e+02],\n",
       "       [ -9.82548933e+00,  -5.65406200e+01],\n",
       "       [  1.68635202e+01,   2.91454059e+00],\n",
       "       [  8.81418809e+01,   1.01909894e+02],\n",
       "       [ -6.04751775e+01,   4.64119434e+01],\n",
       "       [ -1.09522850e+02,  -3.10225124e+01],\n",
       "       [  1.75280666e+02,  -1.72156885e+02],\n",
       "       [ -3.15520676e-01,   1.23625103e+01],\n",
       "       [  5.17495776e+01,   5.00387848e+01],\n",
       "       [  6.67124959e+01,  -1.40781122e+01],\n",
       "       [  1.04544308e+01,  -5.55394757e+01],\n",
       "       [  1.16202764e+02,   6.58743532e+00],\n",
       "       [  8.34268812e+01,  -5.95518782e+01],\n",
       "       [  1.46335308e+01,   4.96053348e+01],\n",
       "       [  4.14790138e+01,  -8.44940654e+00],\n",
       "       [ -6.58333755e+01,  -3.39320675e+00],\n",
       "       [  3.19777153e+01,   9.81307738e+01],\n",
       "       [ -7.51736808e+00,   6.79392414e+01],\n",
       "       [ -5.09587469e+01,  -6.45178343e+01],\n",
       "       [ -7.83759110e+01,   2.05116857e+01],\n",
       "       [ -7.13230388e+01,   6.72160882e+01],\n",
       "       [ -5.77130297e+01,  -3.31153050e+01],\n",
       "       [ -3.10141971e+01,  -6.78364682e+01],\n",
       "       [ -8.81353910e+01,   8.00327123e+01],\n",
       "       [ -1.24799839e+02,   2.18916215e+02],\n",
       "       [ -2.58037516e+01,   9.75093601e+01],\n",
       "       [  6.12753852e+01,   8.52610545e+01],\n",
       "       [ -3.71945149e+01,  -4.07454801e+01],\n",
       "       [  1.80979885e-01,   1.09867505e+02],\n",
       "       [  1.45950675e+01,  -8.15942474e+01],\n",
       "       [ -1.20547568e+01,  -1.11262251e+02],\n",
       "       [  3.64320415e+01,   9.13403513e+00],\n",
       "       [ -5.56159456e+01,   9.00916426e+01],\n",
       "       [ -6.36655378e+01,  -9.00384641e+01],\n",
       "       [ -8.25715622e+01,  -1.06107889e+02],\n",
       "       [  3.27140970e+01,  -5.47280681e+01],\n",
       "       [ -1.43163251e+01,   4.02036924e+01],\n",
       "       [ -2.38749673e+01,   1.73242445e+01],\n",
       "       [  6.12237737e+01,  -7.00066724e+01],\n",
       "       [  1.29205837e+01,   7.44622718e+01],\n",
       "       [ -2.67205653e+01,   5.69542700e+01],\n",
       "       [  5.15411347e+01,  -4.64151807e+01],\n",
       "       [ -2.19882408e+01,  -8.72448676e+01],\n",
       "       [  7.77121691e+00,  -2.65731320e+01],\n",
       "       [  5.58802562e+00,   3.31125729e+01],\n",
       "       [ -5.16833975e+01,   1.68521479e+01],\n",
       "       [  5.64277761e+01,   7.26737746e+00],\n",
       "       [  8.50786538e+01,  -2.76177129e+01]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents_tsne = []\n",
    "sentence_rep = []\n",
    "sentence_rep = get_sentence_embedding(embedding_matrix)\n",
    "intents_tsne = (tsne.fit_transform(sentence_rep))\n",
    "intents_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = figure(tools=\"pan,wheel_zoom,reset,save\",\n",
    "           toolbar_location=\"above\",\n",
    "           title=\"Glove T-SNE for dialogue intents\")\n",
    "\n",
    "source = ColumnDataSource(data=dict(x1=intents_tsne[:,0],\n",
    "                                    x2=intents_tsne[:,1],\n",
    "                                    names=word_index))\n",
    "\n",
    "p.scatter(x=\"x1\", y=\"x2\", size=6, source=source)\n",
    "\n",
    "labels = LabelSet(x=\"x1\", y=\"x2\", text=\"names\", y_offset=10,\n",
    "                  text_font_size=\"8pt\", text_color=\"#555555\",\n",
    "                  source=source, text_align='center')\n",
    "p.add_layout(labels)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To Do:\n",
    "1. SVO Triplets\n",
    "2. PCA "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
